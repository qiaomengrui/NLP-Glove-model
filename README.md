# Glove模型
**Glove模型目的：求出能表示出词与词之间的关系的词向量   
Glove的整体思路：通过对词的共现计数矩阵进行降维，来得到词向量**
## 一、 模型原理
模型实现：根据整个语料建立一个大型的体现词共现情况的矩阵，其目标是优化减小重建损失，即降维之后的向量能尽量表达原始向量的完整信息
### 1. 共现矩阵  
共现矩阵的概率比值可以用来区分词（共现：指语料中词汇一起出现的频率）  
共现矩阵的每一行分别代表每个词的词向量  
假设样本数据集：
1. I like deep learning.   
2. I like NLP.   
3. I enjoy flying.  

通过统计词的出现次数绘出V\*V共现次数矩阵，如下图  
![共现矩阵](https://github.com/qiaomengrui/NLP-Glove-model/blob/master/pic/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5.png)  
缺点：  
当词汇量过大时，向量就会越大，矩阵会呈现更高的维度，同时需要更大的存储空间，而矩阵也会有稀疏的部分，这时就需要降维处理
### 2. 目标函数
目标函数经过推导得到的最终结果如下图  
![目标函数](https://github.com/qiaomengrui/NLP-Glove-model/blob/master/pic/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5.png)  
参数解释：
* Xij：表示词j出现在中心词i的上下文的次数  
* Xi：表示任何词出现在中心词i的上下文的次数
* wi：表示i的词向量
* f(Xij)：表示权重函数
* bi：表示偏置
* V：表示词的总数（去重）
### 3. 权重函数
权重函数相当于起降维的作用  
权重函数的设计需满足以下三点：
* f(0)=0，如果两个词没有共同出现过，权重为0，不参与训练
* 两个词出现的次数多，权重不能变小
* 对于高频词并且包含信息极少的词，不能取过高的值  

根据这三个特点，构造了权重函数，最终结果如下图  
![权重函数](https://github.com/qiaomengrui/NLP-Glove-model/blob/master/pic/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5.png)  

当超参数xmax=100，a=0.75，权重函数的图像如下  
![给参数的权重函数图像](https://github.com/qiaomengrui/NLP-Glove-model/blob/master/pic/%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5.png)  
## 二. Glove相关知识
### 1. SVD
SVD是一种降维方式  
但是SVD也有缺陷：
1. 计算代价太大  
2. 难以将新的词合并进去  
## 三. Glove和word2vec比较
1. 整体上来看Glove相当于skip-gram的一种特殊形式，Glove的权重函数设置更加合理，所以训练效果更加好，权重函数就是skip-gram中对单词进行重采样处理  
2. 两个模型都可以根据词汇的共现矩阵，将词汇编码成一个向量  
3. word2vec基于预测的模型，其目标是不断提高对其他词的预测能力，即减小预测损失，从而得到词向量    
   Glove基于统计的模型，是通过对词的共现计数矩阵进行降维，来得到词向量。  
实际使用中，两种向量的对下游任务的效果并没有太大差别。
4. Glove相对于word2vec有一个优点是更容易并行化执行，可以更快，更容易地在大规模语料上训练
## 四、Glove总结

